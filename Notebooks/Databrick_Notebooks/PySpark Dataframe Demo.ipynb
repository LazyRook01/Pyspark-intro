{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "053b2178-aadf-455e-ada0-e45fcb63e489",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySpark Example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99931b5b-10b4-48e2-86e4-7d558ad31569",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here is an explanation of each part:\n",
    "\n",
    "1. `from pyspark.sql import SparkSession`: This imports the SparkSession class from the pyspark.sql module. SparkSession is the entry point to programming Spark with the Dataset and DataFrame API.\n",
    "\n",
    "2. `spark = SparkSession.builder`: This creates a SparkSessionBuilder, which is used to configure the session.\n",
    "\n",
    "3. `appName(\"PySpark Example\")`: This method sets the name of the Spark application, which will be visible in the Spark web UI.\n",
    "\n",
    "4. `config(\"spark.some.config.option\", \"some-value\")`: This method is used to set various Spark properties. Here, it sets a configuration option with the key \"spark.some.config.option\" and the value \"some-value\". You can configure various properties depending on your use case.\n",
    "\n",
    "5. `getOrCreate()`: This method gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in the builder.\n",
    "\n",
    "In summary, this code sets up a Spark session with the specified configuration and properties, allowing you to leverage the capabilities of the Spark SQL module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7915859d-14c4-4679-8b6f-d3ce26592ceb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=6010360890755387#setting/sparkui/1024-141522-uewlz0do/driver-6344801681206063526\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1efc293820>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1022731b-1319-41f1-9ab3-d3dd86b8d4ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Load a csv file as Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8f9dae2-43e8-4b65-aea9-09c206da6a4f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/gaurav.ojha008@nmims.edu.in/bank_full-1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9634540-c036-44dd-b78b-4897d5c1335a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[age: string, job: string, marital: string, education: string, default: string, balance: string, housing: string, loan: string, contact: string, day: string, month: string, duration: string, campaign: string, pdays: string, previous: string, poutcome: string, Target: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a4df613-08ff-44da-85ca-bc28e6e3f2d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Layout of the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51c2a4ad-4417-4bdc-9903-802095952420",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- age: string (nullable = true)\n |-- job: string (nullable = true)\n |-- marital: string (nullable = true)\n |-- education: string (nullable = true)\n |-- default: string (nullable = true)\n |-- balance: string (nullable = true)\n |-- housing: string (nullable = true)\n |-- loan: string (nullable = true)\n |-- contact: string (nullable = true)\n |-- day: string (nullable = true)\n |-- month: string (nullable = true)\n |-- duration: string (nullable = true)\n |-- campaign: string (nullable = true)\n |-- pdays: string (nullable = true)\n |-- previous: string (nullable = true)\n |-- poutcome: string (nullable = true)\n |-- Target: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48e5e7d2-e299-4df9-b8db-c75ca9ac830d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Reading a Parquet File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8a2a3e7-348e-4e96-8d99-d90766d9e5f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = spark.read.format(\"parquet\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/gaurav.ojha008@nmims.edu.in/userdata1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd798a4a-744a-4f61-9f2c-3114f71755e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[registration_dttm: timestamp, id: int, first_name: string, last_name: string, email: string, gender: string, ip_address: string, cc: string, country: string, birthdate: string, salary: double, title: string, comments: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72a3c2a2-dcef-4751-a3da-5612fb04b7cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- registration_dttm: timestamp (nullable = true)\n |-- id: integer (nullable = true)\n |-- first_name: string (nullable = true)\n |-- last_name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- ip_address: string (nullable = true)\n |-- cc: string (nullable = true)\n |-- country: string (nullable = true)\n |-- birthdate: string (nullable = true)\n |-- salary: double (nullable = true)\n |-- title: string (nullable = true)\n |-- comments: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba5a715e-cf98-401b-9f3e-d73f0fcaa5a5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Get the number of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d398f69f-0bc7-4336-83ca-c9dc7c50b07c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark Dataframe Demo",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
